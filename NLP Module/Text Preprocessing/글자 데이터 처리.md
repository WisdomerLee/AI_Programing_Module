# 글자 데이터를 AI, 컴퓨터가 이해할 수 있도록 변환하기 전에 글자를 처리하는 과정
글자의 불필요한 부분을 없애고, 사용자의 의도를 반영하기 쉽게 하도록 데이터화하는 과정으로
해당 과정이 없이 진행되면, 사용자의 의도와 무관하게 언급된 단어가 중요함을 갖게 되는 등의 문제를 방지할 수 있도

# Lowercase로 바꾸기(소문자)- 영어쪽이 해당
```python
tinker = "Tinker".tolower()
```

# the와 같은 의미 없는 글자들 없애기(stop word)
```python
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
# 영어의 stopword들의 목록을 얻고 싶다면 아래와 같이...
en_stopwords = stopwords.words('english')

sentence = ""
sentence_no_stopwords = ' '.join([word for word in sentence.split() if word not in en_stopwords])

# 아래와 같이 stopword를 제거할 수도 있음
en_stopwords.remove("did")
en_stopwords.remove("not")
# 아래와 같이 stopword를 추가할 수 있음
en_stopwords.append("go")


sentence_no_stopwords_custom = ' '.join([word for word in sentence.split() if word not in en_stopwords])
```

# Regular expression
```python
import re


```
글자에서 패턴을 찾아 해당 패턴을 수정 혹은 삭제 하는 등의 방식으로 불필요한 내용을 수정, 제거

# Tokenization
AI가 이해하는 단위 token으로 문자를 쪼개는 과정
한 번에 여러 데이터를 집어넣을 수 없기 때문에 고안된 부분
```python
import nltk
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize, sent_tokenize

sentences = ""
sent_tokenize(sentences)
sentence = ""
word_tokenize(sentence)
sentence2 = ""
workd_tokenize(sentence)
```

# Stemming
단어를 기본의 형태로 변경
고유 단어들을 줄이고, 같은 단어를 같은 표현식으로 변경하여 주기 때문에 맥락상의 이해도를 더 높일 수 있음

```python
from nltk.stem import PorterStemmer

ps = PorterStemmer()
connect_tokens= ["connecting", "connected", "connectivity", "connect", "connects"]

for t in connect_tokens:
  print(t, ": ", ps.stem(t))


```

# Lemmatization

```python
import nltk
nltk.download('wordnet)
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
for t in connect_tokens:
  print(t, ": ", lemmatizer.lemmatize(t))

```

# N-grams
```python

```
